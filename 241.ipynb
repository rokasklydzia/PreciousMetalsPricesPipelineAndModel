{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9623548-4fa0-4fa6-9db4-8216e53622d1",
   "metadata": {},
   "source": [
    "# Module 2: Fundamentals of Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28558f-f42f-4b65-a610-660e76698e07",
   "metadata": {},
   "source": [
    "## Sprint 4: Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f13880-047c-4340-a8e3-a71c02948550",
   "metadata": {},
   "source": [
    "# About this Sprint\n\nIt is time for your second Capstone Project.  \nYou will work on this project for the whole Sprint.  \nThe outcome of this Sprint should potentially serve as your portfolio item, so try to show your best work!\n\nThis time your objective is even more challenging - you will be required to iteratively build and implement a plan for a large dataset based on business objectives.  \nYou'll have to translate the business requirements, making assumptions where necessary, into a plan for your project.\n\nEven though you have learned a lot about Python, SQL, Apache Airflow in this Sprint, working on a larger project will be the real challenge.\n\nGood luck!\n\n## Context\n\nYou are hired by a hedge fund specializing in using the latest technologies to develop bespoke trading strategies to gain an edge in trading various financial instruments.  \nYou are working in a team that is using machine learning to predict the price movements of commodities - oil, natural gas, gold, silver, etc.  \nYour role in the team is to help your team release the precious metals machine learning models that they have been developing for the past year.\n\n- Your database should contain at least two schema objects (tables, views, etc.) - one for training machine learning models and another for analytical workflows.\n\nDuring the planning session you have committed to deliver these tasks:\n- Set up an RDBMS that will have the required data to be used for both model training and analytical workflows. The database objects that you use should contain at minimum columns for each of the metals, a timestamp, and an id. The table used for machine learning training should only contain rows for the last twelve hourly entries, as this is how much data is required for training the ML model. The table used for analytics should contain all available data.\n- Set up a data ingestion solution for gold, silver, platinum, and palladium prices. You can use any API that you'd like. Some suggestions for the APIs: [Live Metal Prices](https://rapidapi.com/solutionsbynotnull/api/live-metal-prices), [Gold & Silver Prices](https://www.goldapi.io/), [Metals-API](https://metals-api.com/documentation), [api.metals.live](http://api.metals.live/), [Metalprice API](https://metalpriceapi.com/pricing). P.S. you should not need to use a paid plan for this project.\n- Modify the Model class to connect to the real data sources and use them for trainig the models.\n- Set up the machine learning training pipeline, which should result in multiple trained model files.\n- Set up periodical backups of the machine learning models and the database.\n- Keep all historical data available for analytical purposes.\n\nNote: for the RDBMS, you can use any technology that you like (e.g., IBM Db2, PostgreSQL, etc.), and or orchestrating your jobs use Apache Airflow.\n\n### Model Definiton\n\nThis is the code for training machine learning models that the data scientists in you team have given you to test your pipelines.\nFor reference, it includes a way to generate data so that you know how to structure your data, when you connect a real data source to the model.\nTheir final Model are going to be different, but the API will be the same, so make your solution modular.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sktime.forecasting.arima import ARIMA\n\nrng = np.random.default_rng()\n\nAR_LOWER = 0.1\nAR_UPPER = 0.6\nMEAN_LOWER = 1000\nMEAN_UPPER = 2000\nSTD = 1\n\n\ndef generate_integrated_autocorrelated_series(\n    p: float, mean: float, std: float, length: int\n) -> np.ndarray:\n    \"\"\"Generates an integrated autocorrelated time series using a specified autoregression parameter, mean and standard deviation of the normal distribution, and the desired length of the series.\"\"\"\n    x = 0\n    ar1_series = np.asarray([x := p * x + rng.normal(0, 1) for _ in range(length)])\n    return np.cumsum(ar1_series * std) + mean\n\n\ndef generate_sample_data(\n    cols: list[str], x_size: int, y_size: int\n) -> tuple[pd.DataFrame, pd.DataFrame, tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Generates sample training and test data for specified columns. The data consists of autocorrelated series, each created with randomly generated autoregression coefficients and means. The method also returns the generated autocorrelation coefficients and means for reference. 'x_size' determines the length of the training set, and 'y_size' determines the length of the test set. 'cols' determines the names of the columns.\"\"\"\n    ar_coefficients = rng.uniform(AR_LOWER, AR_UPPER, len(cols))\n    means = rng.uniform(MEAN_LOWER, MEAN_UPPER, len(cols))\n    full_dataset = pd.DataFrame.from_dict(\n        {\n            col_name: generate_integrated_autocorrelated_series(\n                ar_coefficient, mean, STD, x_size + y_size\n            )\n            for ar_coefficient, mean, col_name in zip(ar_coefficients, means, cols)\n        }\n    )\n    return (\n        full_dataset.head(x_size),\n        full_dataset.tail(y_size),\n        (ar_coefficients, means),\n    )\n\n\nclass Model:\n    def __init__(self, tickers: list[str], x_size: int, y_size: int) -> None:\n        self.tickers = tickers\n        self.x_size = x_size\n        self.y_size = y_size\n        self.models: dict[str, ARIMA] = {}\n\n    def train(self, /, use_generated_data: bool = False) -> None:\n        if use_generated_data:\n            data, _, _ = generate_sample_data(\n                self.tickers, self.x_size, self.y_size\n            )\n        else:\n            raise NotImplementedError\n        for ticker in self.tickers:\n            dataset = data[ticker].values\n            model = ARIMA(order=(1, 1, 0), with_intercept=True, suppress_warnings=True)\n            model.fit(dataset)\n            self.models[ticker] = model\n\n    def save(self, path_to_dir: str | Path) -> None:\n        path_to_dir = Path(path_to_dir)\n        path_to_dir.mkdir(parents=True, exist_ok=True)\n        for ticker in self.tickers:\n            full_path = path_to_dir / ticker\n            self.models[ticker].save(full_path)\n```\n\n### Training and Saving the Model\n\nThe data scientists also said that you can use this script to see how the model training and saving works with generated data.\n\n```python\nmodel = Model([\"XAUUSD\", \"XAGUSD\", \"XPTUSD\", \"XPDUSD\"], 12, 1)\nmodel.train(use_generated_data=True)\nmodel.save(\"model1\")\n```\n\n### Dependencies\n\nEven when you asked about dependencies (Python version, 3rd party libraries, etc.) for this code, the data scientist couldn't give you a good answer - they didn't remember which versions of the Python they used in their notebooks.\nThey said that they simply ran `pip install sktime` and other libraries were already installed in the environment that they used.\nYou will need to figure out the dependencies for this pipeline yourself.\n\n## Objectives for this Part\n\n- Practice translating business requirements into data engineering tasks.\n- Practice architecting a solution feeding data to a Python application.\n- Practice orchestrating jobs using Apache Airflow.\n- Practice using machine learning to solve business problems.\n- Practice deploying multiple machine learning models.\n\n## Requirements\n\n- Your solution should encompass the functionality outlined in the Context section.\n- Create a plan for your deliveries. This should include your assumptions, overall objectives, and objectives for each step in your plan. You are not expected to have a plan for the whole project but instead have a clear understanding of what you'll try to achieve in the next step and build the plan one step at a time.\n- Architect a solution enabling the machine learning models to use the data in your database.\n- Your machine learning training pipeline should be scheduled to run immediately after loading the data from the APIs.\n- Your system should backup the entire database and the machine learning models every six hours and store the last twenty backups.\n- Provide suggestions about how your analysis can be improved.\n\n## Bonus Challenges\n\nAs a data engineer, you will spend a significant amount of your time learning new things.  \nSometimes you will do that for fun, but most of the time, you will have an urgent problem, and you will need to quickly learn some new skills to be able to solve it.  \nIt is essential to build this skill gradually - it is extremely valuable for all data engineers.  \nThe bonus challenges are designed to simulate these types of situations.  \nThese challenges require you to do something that we haven't covered in the course yet.  \nInstead of trying to do all of the bonus challenges, concentrate on just one or two and do them well.  \nAll of the bonus challenges are optional - no points will be deducted if you skip them.\n\n- Write unit and integration tests for your solution.\n- Extend your solution to store all hyperparamters and parameters of each model for each training run in a separate table or database.\n\n## Evaluation Criteria\n\n- Adherence to the requirements. How well did you meet the requirements?\n- Code quality. Was your code well-structured? Did you use the appropriate levels of abstraction? Did you remove commented-out and unused code?\n- Code performance. Did you use suitable algorithms and data structures to solve the problems?\n- Presentation quality. Coherence of the presentation of the project, how well everything is explained.\n- General understanding of the topic.\n\n## Correction\n\nDuring your project correction, you should present it as if talking to a data scientist building the machine learning model in your team.  \nYou can assume that they will have strong data science and decent software engineering skills - they will understand technical jargon but are not expected to notice things that could have been done better or ask about the choices you've made.\nThey are well familiar with the problem (but it is always worth to recap of which part of the problem you were asked to solve), so don't spend your time explaining trivial concepts or code snippets that are simple - your best bet is to focus your presentation on technological and design choices as well as the end-user functionality of your solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42d1c9-0c57-4849-8666-9a2ec4f6d3ab",
   "metadata": {},
   "source": [
    "## General Correction Guidelines\n\nFor an in-depth explanation about how corrections work at Turing College, please read [this doc](https://turingcollege.atlassian.net/wiki/spaces/DLG/pages/537395951/Peer+expert+reviews+corrections).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
